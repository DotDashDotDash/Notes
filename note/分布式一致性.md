# 分布式一致性

## 什么是一致性

* 弱一致性
  * 最终一致性: DNS, Gossip
* 强一致性
  * 同步
  * Paxos
  * Raft(Multi-Paxos)
  * ZAB(Multi-Paxos)
  
## 强一致性算法

1. 主从同步复制

* Master接受写请求
* Master复制日志至Slave
* Master等待，直到**所有**从库中返回(这一步可能会产生问题，Master阻塞，导致整个集群不可用，虽然保证了一致性，但是集群的可用性却是大大降低)

2. 多数派

每次写都保证写大于N/2个节点，每次读保证从N/2个节点中读

多数派产生的问题:**在并发环境下，无法保证系统的正确性，顺序非常重要**

3. Paxos

* Basic Paxos
* Multi Paxos
* Fast Paxos

## Basic Paxos

### 步骤阶段

* Phase 1a: Prepare: proposer提出一个提案，编号为N，若N大于proposer之前提出的提案编号，请求acceptor的quorum接受
* Phase 1b: Promise: 若N大于此acceptor之前接受的任何提案编号，则**接受**，否则拒绝
* Phase 2a: Accept: 如果达到了多数派，proposer会发出accept请求，此请求包含提案编号N，以及提案内容
* Phase 2b: Accepted: 如果此acceptor在此期间没有收到任何编号大于N的提案，则接受此提案内容，否则忽略

### 失败节点

* Acceptor节点失败，若达到quorum，仍然可以达到一致性，体现了容错率
* Proposer节点失败，会有另外一个Proposer来继续完成前一个失败Proposer节点的任务

### Basic Paxos问题

难实现，效率低(2轮RPC)，活锁

## Multi Paxos

### 与Basic Paxos不同

* 出现Leader，所有的请求都必须经过Leader
* 只有在选举Leader的时候，才需要经过两轮RPC
* 如果Leader失效，需要重新选举Leader
* 选举出Leader之后，服务器的角色就分成了一个Leader和若干个Slave

## Raft

### Raft将分布式一致性分为三个子问题

* Leader Election: 多数派选举，通过心跳来维持联系，一个节点没有收到心跳，它就成为Candidate，即要选举自己成为Leader，等待他人投票，如果Leader失效，最先到达心跳周期的选举自己成为Leader
* Log Replication: 多数派Slave写入，写入Leader
* Safety

### Raft重新定义角色

* Leader
* Follower
* Candidate

### 模拟Raft的网站

* [https://raft.github.io]

## 强一致性算法ZAB

与Raft相似，只不过ZAB将一个Leader的任期称为epoch，而Raft称为term
